# INTELIGENCIA ARTIFICIAL
Contenido del Curso de Especialización

---

## **BLOQUE 1: FUNDAMENTOS MATEMÁTICOS Y DE MACHINE LEARNING**

### **Tema 1.1: Matemáticas para IA - Álgebra Lineal**
- **Lección 1.1.1:** Vectores y espacios vectoriales
- **Lección 1.1.2:** Matrices y operaciones matriciales
- **Lección 1.1.3:** Producto escalar y normas vectoriales
- **Lección 1.1.4:** Matrices especiales: identidad, diagonal, transpuesta
- **Lección 1.1.5:** Determinantes y matrices inversas
- **Lección 1.1.6:** Autovalores y autovectores
- **Lección 1.1.7:** Descomposición de matrices (SVD, eigendecomposition)
- **Lección 1.1.8:** Tensores y operaciones tensoriales

### **Tema 1.2: Matemáticas para IA - Cálculo**
- **Lección 1.2.1:** Derivadas y reglas de derivación
- **Lección 1.2.2:** Derivadas parciales
- **Lección 1.2.3:** Gradientes y vectores gradiente
- **Lección 1.2.4:** Regla de la cadena
- **Lección 1.2.5:** Optimización: máximos y mínimos
- **Lección 1.2.6:** Descenso de gradiente
- **Lección 1.2.7:** Jacobiano y Hessiano

### **Tema 1.3: Probabilidad y estadística**
- **Lección 1.3.1:** Conceptos básicos de probabilidad
- **Lección 1.3.2:** Variables aleatorias y distribuciones
- **Lección 1.3.3:** Distribuciones comunes: normal, binomial, Poisson
- **Lección 1.3.4:** Teorema de Bayes y probabilidad condicional
- **Lección 1.3.5:** Esperanza, varianza y covarianza
- **Lección 1.3.6:** Inferencia estadística
- **Lección 1.3.7:** Pruebas de hipótesis
- **Lección 1.3.8:** Intervalos de confianza

### **Tema 1.4: Teoría de la información**
- **Lección 1.4.1:** Entropía de Shannon
- **Lección 1.4.2:** Entropía cruzada
- **Lección 1.4.3:** Divergencia KL (Kullback-Leibler)
- **Lección 1.4.4:** Ganancia de información
- **Lección 1.4.5:** Aplicaciones en Machine Learning

### **Tema 1.5: Introducción a la Inteligencia Artificial**
- **Lección 1.5.1:** Historia de la IA: desde Turing hasta la actualidad
- **Lección 1.5.2:** Tipos de IA: débil vs fuerte
- **Lección 1.5.3:** ANI, AGI y ASI
- **Lección 1.5.4:** Aplicaciones actuales de la IA
- **Lección 1.5.5:** El futuro de la IA

### **Tema 1.6: Paradigmas de aprendizaje**
- **Lección 1.6.1:** Aprendizaje supervisado: conceptos y casos de uso
- **Lección 1.6.2:** Aprendizaje no supervisado: conceptos y casos de uso
- **Lección 1.6.3:** Aprendizaje semi-supervisado
- **Lección 1.6.4:** Aprendizaje por refuerzo: introducción
- **Lección 1.6.5:** Comparativa de paradigmas

### **Tema 1.7: Preprocesamiento de datos**
- **Lección 1.7.1:** Limpieza de datos y manejo de valores faltantes
- **Lección 1.7.2:** Normalización y estandarización
- **Lección 1.7.3:** Encoding de variables categóricas
- **Lección 1.7.4:** Manejo de datos desbalanceados
- **Lección 1.7.5:** Técnicas de oversampling y undersampling
- **Lección 1.7.6:** SMOTE y variantes

### **Tema 1.8: Feature Engineering**
- **Lección 1.8.1:** Concepto y importancia del Feature Engineering
- **Lección 1.8.2:** Creación de nuevas características
- **Lección 1.8.3:** Transformaciones de características
- **Lección 1.8.4:** Feature Selection: métodos de filtrado
- **Lección 1.8.5:** Feature Selection: métodos wrapper
- **Lección 1.8.6:** Feature Selection: métodos embedded
- **Lección 1.8.7:** Extracción automática de características

### **Tema 1.9: Métricas de evaluación**
- **Lección 1.9.1:** Métricas para clasificación: accuracy, precision, recall
- **Lección 1.9.2:** F1-Score y F-beta Score
- **Lección 1.9.3:** Matrices de confusión
- **Lección 1.9.4:** Curvas ROC y AUC
- **Lección 1.9.5:** Métricas para regresión: MSE, RMSE, MAE, R²
- **Lección 1.9.6:** Métricas para problemas multi-clase
- **Lección 1.9.7:** Métricas personalizadas según el negocio

### **Tema 1.10: Validación de modelos**
- **Lección 1.10.1:** División train/test/validation
- **Lección 1.10.2:** Validación cruzada (k-fold)
- **Lección 1.10.3:** Validación cruzada estratificada
- **Lección 1.10.4:** Leave-One-Out Cross Validation
- **Lección 1.10.5:** Time series split para datos temporales

### **Tema 1.11: Conceptos fundamentales de ML**
- **Lección 1.11.1:** Bias-variance tradeoff
- **Lección 1.11.2:** Overfitting y underfitting
- **Lección 1.11.3:** Capacidad del modelo y complejidad
- **Lección 1.11.4:** No Free Lunch Theorem
- **Lección 1.11.5:** Maldición de la dimensionalidad

### **Tema 1.12: Introducción a scikit-learn**
- **Lección 1.12.1:** Instalación y configuración de scikit-learn
- **Lección 1.12.2:** API de scikit-learn: estimators, transformers, predictors
- **Lección 1.12.3:** Pipelines en scikit-learn
- **Lección 1.12.4:** Preprocesamiento con scikit-learn
- **Lección 1.12.5:** GridSearchCV y RandomizedSearchCV

---

## **BLOQUE 2: MACHINE LEARNING CLÁSICO**

### **Tema 2.1: Regresión lineal**
- **Lección 2.1.1:** Fundamentos de regresión lineal
- **Lección 2.1.2:** Mínimos cuadrados ordinarios (OLS)
- **Lección 2.1.3:** Supuestos de la regresión lineal
- **Lección 2.1.4:** Interpretación de coeficientes
- **Lección 2.1.5:** Diagnóstico de residuos
- **Lección 2.1.6:** Implementación con scikit-learn

### **Tema 2.2: Regresión regularizada**
- **Lección 2.2.1:** Regresión Ridge (L2)
- **Lección 2.2.2:** Regresión Lasso (L1)
- **Lección 2.2.3:** Elastic Net
- **Lección 2.2.4:** Selección del parámetro de regularización
- **Lección 2.2.5:** Comparativa de métodos de regularización

### **Tema 2.3: Regresión logística**
- **Lección 2.3.1:** Fundamentos de regresión logística
- **Lección 2.3.2:** Función sigmoide y odds ratio
- **Lección 2.3.3:** Máxima verosimilitud
- **Lección 2.3.4:** Regresión logística multi-clase
- **Lección 2.3.5:** Interpretación de resultados
- **Lección 2.3.6:** Regularización en regresión logística

### **Tema 2.4: Árboles de decisión**
- **Lección 2.4.1:** Fundamentos de árboles de decisión
- **Lección 2.4.2:** Algoritmo ID3 y C4.5
- **Lección 2.4.3:** Criterios de división: Gini y entropía
- **Lección 2.4.4:** Poda de árboles
- **Lección 2.4.5:** Árboles de regresión (CART)
- **Lección 2.4.6:** Interpretabilidad de árboles
- **Lección 2.4.7:** Implementación práctica

### **Tema 2.5: Random Forest**
- **Lección 2.5.1:** Concepto de ensemble learning
- **Lección 2.5.2:** Bagging y bootstrap aggregating
- **Lección 2.5.3:** Random Forest: arquitectura
- **Lección 2.5.4:** Feature importance en Random Forest
- **Lección 2.5.5:** Hiperparámetros y optimización
- **Lección 2.5.6:** Out-of-bag error

### **Tema 2.6: Gradient Boosting - XGBoost**
- **Lección 2.6.1:** Fundamentos de boosting
- **Lección 2.6.2:** Gradient Boosting: algoritmo
- **Lección 2.6.3:** XGBoost: arquitectura y optimizaciones
- **Lección 2.6.4:** Regularización en XGBoost
- **Lección 2.6.5:** Hiperparámetros críticos
- **Lección 2.6.6:** Early stopping y cross-validation

### **Tema 2.7: LightGBM y CatBoost**
- **Lección 2.7.1:** LightGBM: arquitectura y ventajas
- **Lección 2.7.2:** Gradient-based One-Side Sampling (GOSS)
- **Lección 2.7.3:** CatBoost: manejo de variables categóricas
- **Lección 2.7.4:** Ordered boosting
- **Lección 2.7.5:** Comparativa: XGBoost vs LightGBM vs CatBoost

### **Tema 2.8: Support Vector Machines (SVM)**
- **Lección 2.8.1:** Fundamentos de SVM
- **Lección 2.8.2:** Margen máximo y vectores de soporte
- **Lección 2.8.3:** Kernel trick y kernels comunes
- **Lección 2.8.4:** SVM para clasificación multi-clase
- **Lección 2.8.5:** SVM para regresión (SVR)
- **Lección 2.8.6:** Selección de hiperparámetros

### **Tema 2.9: K-Nearest Neighbors (KNN)**
- **Lección 2.9.1:** Fundamentos de KNN
- **Lección 2.9.2:** Métricas de distancia: Euclidiana, Manhattan, Minkowski
- **Lección 2.9.3:** Selección de k óptimo
- **Lección 2.9.4:** KNN ponderado
- **Lección 2.9.5:** Optimizaciones: KD-trees y Ball-trees
- **Lección 2.9.6:** KNN para regresión

### **Tema 2.10: Naive Bayes**
- **Lección 2.10.1:** Teorema de Bayes aplicado a clasificación
- **Lección 2.10.2:** Supuesto de independencia condicional
- **Lección 2.10.3:** Gaussian Naive Bayes
- **Lección 2.10.4:** Multinomial Naive Bayes
- **Lección 2.10.5:** Bernoulli Naive Bayes
- **Lección 2.10.6:** Aplicaciones en clasificación de texto

### **Tema 2.11: Clustering - K-means**
- **Lección 2.11.1:** Conceptos de clustering
- **Lección 2.11.2:** Algoritmo K-means
- **Lección 2.11.3:** Método del codo para seleccionar k
- **Lección 2.11.4:** Silhouette score
- **Lección 2.11.5:** K-means++: inicialización mejorada
- **Lección 2.11.6:** Mini-batch K-means

### **Tema 2.12: Clustering jerárquico y DBSCAN**
- **Lección 2.12.1:** Clustering jerárquico aglomerativo
- **Lección 2.12.2:** Dendrogramas
- **Lección 2.12.3:** Métodos de enlace: single, complete, average
- **Lección 2.12.4:** DBSCAN: density-based clustering
- **Lección 2.12.5:** Parámetros eps y min_samples
- **Lección 2.12.6:** Detección de outliers con DBSCAN

### **Tema 2.13: Otros algoritmos de clustering**
- **Lección 2.13.1:** Gaussian Mixture Models (GMM)
- **Lección 2.13.2:** Mean Shift clustering
- **Lección 2.13.3:** Spectral clustering
- **Lección 2.13.4:** Comparativa de algoritmos de clustering

### **Tema 2.14: Reducción de dimensionalidad - PCA**
- **Lección 2.14.1:** Motivación y aplicaciones
- **Lección 2.14.2:** Principal Component Analysis (PCA)
- **Lección 2.14.3:** Varianza explicada
- **Lección 2.14.4:** PCA incremental y Kernel PCA
- **Lección 2.14.5:** Interpretación de componentes principales

### **Tema 2.15: t-SNE y UMAP**
- **Lección 2.15.1:** t-SNE: fundamentos
- **Lección 2.15.2:** Perplexity y parámetros de t-SNE
- **Lección 2.15.3:** UMAP: Uniform Manifold Approximation
- **Lección 2.15.4:** Comparativa t-SNE vs UMAP vs PCA
- **Lección 2.15.5:** Visualización de embeddings

### **Tema 2.16: Detección de anomalías**
- **Lección 2.16.1:** Conceptos de detección de anomalías
- **Lección 2.16.2:** Isolation Forest
- **Lección 2.16.3:** One-Class SVM
- **Lección 2.16.4:** Local Outlier Factor (LOF)
- **Lección 2.16.5:** Aplicaciones en fraud detection

### **Tema 2.17: Hyperparameter tuning avanzado**
- **Lección 2.17.1:** Grid Search exhaustivo
- **Lección 2.17.2:** Random Search
- **Lección 2.17.3:** Bayesian Optimization
- **Lección 2.17.4:** Hyperband y BOHB
- **Lección 2.17.5:** Optuna para optimización de hiperparámetros

### **Tema 2.18: Integración con Spark MLlib**
- **Lección 2.18.1:** ML con datasets masivos
- **Lección 2.18.2:** Spark MLlib: arquitectura
- **Lección 2.18.3:** Algoritmos distribuidos en Spark
- **Lección 2.18.4:** Pipelines de ML con Spark
- **Lección 2.18.5:** Feature engineering a gran escala

---

## **BLOQUE 3: DEEP LEARNING Y REDES NEURONALES**

### **Tema 3.1: Fundamentos de redes neuronales**
- **Lección 3.1.1:** Inspiración biológica de las redes neuronales
- **Lección 3.1.2:** El perceptrón
- **Lección 3.1.3:** Perceptrón multicapa (MLP)
- **Lección 3.1.4:** Arquitectura de redes neuronales
- **Lección 3.1.5:** Forward propagation

### **Tema 3.2: Funciones de activación**
- **Lección 3.2.1:** Función Sigmoide
- **Lección 3.2.2:** Función Tanh
- **Lección 3.2.3:** ReLU (Rectified Linear Unit)
- **Lección 3.2.4:** Leaky ReLU y Parametric ReLU
- **Lección 3.2.5:** ELU y SELU
- **Lección 3.2.6:** Swish y GELU
- **Lección 3.2.7:** Softmax para clasificación multi-clase

### **Tema 3.3: Backpropagation**
- **Lección 3.3.1:** Concepto de backpropagation
- **Lección 3.3.2:** Regla de la cadena en redes profundas
- **Lección 3.3.3:** Cálculo de gradientes
- **Lección 3.3.4:** Algoritmo de backpropagation paso a paso
- **Lección 3.3.5:** Computational graphs

### **Tema 3.4: Optimizadores**
- **Lección 3.4.1:** Stochastic Gradient Descent (SGD)
- **Lección 3.4.2:** SGD con Momentum
- **Lección 3.4.3:** Nesterov Accelerated Gradient
- **Lección 3.4.4:** AdaGrad y RMSprop
- **Lección 3.4.5:** Adam optimizer
- **Lección 3.4.6:** AdamW y variantes modernas
- **Lección 3.4.7:** Learning rate scheduling

### **Tema 3.5: Funciones de pérdida**
- **Lección 3.5.1:** Mean Squared Error (MSE)
- **Lección 3.5.2:** Binary Cross-Entropy
- **Lección 3.5.3:** Categorical Cross-Entropy
- **Lección 3.5.4:** Sparse Categorical Cross-Entropy
- **Lección 3.5.5:** Focal Loss
- **Lección 3.5.6:** Funciones de pérdida personalizadas

### **Tema 3.6: Regularización en Deep Learning**
- **Lección 3.6.1:** L1 y L2 regularization
- **Lección 3.6.2:** Dropout: concepto y aplicación
- **Lección 3.6.3:** Dropout espacial y variantes
- **Lección 3.6.4:** Batch Normalization
- **Lección 3.6.5:** Layer Normalization
- **Lección 3.6.6:** Group Normalization
- **Lección 3.6.7:** Early Stopping
- **Lección 3.6.8:** Data Augmentation

### **Tema 3.7: Inicialización de pesos**
- **Lección 3.7.1:** Importancia de la inicialización
- **Lección 3.7.2:** Inicialización aleatoria
- **Lección 3.7.3:** Xavier/Glorot initialization
- **Lección 3.7.4:** He initialization
- **Lección 3.7.5:** Transfer Learning: inicialización con pesos pre-entrenados

### **Tema 3.8: Introducción a TensorFlow y Keras**
- **Lección 3.8.1:** Instalación de TensorFlow
- **Lección 3.8.2:** Tensores y operaciones básicas
- **Lección 3.8.3:** Keras Sequential API
- **Lección 3.8.4:** Keras Functional API
- **Lección 3.8.5:** Custom layers y models
- **Lección 3.8.6:** Callbacks en Keras

### **Tema 3.9: Introducción a PyTorch**
- **Lección 3.9.1:** Instalación de PyTorch
- **Lección 3.9.2:** Tensores en PyTorch
- **Lección 3.9.3:** Autograd: diferenciación automática
- **Lección 3.9.4:** Definición de modelos con nn.Module
- **Lección 3.9.5:** DataLoaders y Datasets
- **Lección 3.9.6:** Training loop en PyTorch

### **Tema 3.10: Redes Neuronales Convolucionales - Fundamentos**
- **Lección 3.10.1:** Concepto de convolución
- **Lección 3.10.2:** Filtros y kernels
- **Lección 3.10.3:** Stride y padding
- **Lección 3.10.4:** Pooling layers: max pooling, average pooling
- **Lección 3.10.5:** Arquitectura básica de una CNN

### **Tema 3.11: Arquitecturas CNN clásicas**
- **Lección 3.11.1:** LeNet-5: la primera CNN exitosa
- **Lección 3.11.2:** AlexNet: revolución en ImageNet
- **Lección 3.11.3:** VGGNet: redes muy profundas
- **Lección 3.11.4:** Inception/GoogLeNet: módulos inception
- **Lección 3.11.5:** ResNet: residual connections
- **Lección 3.11.6:** DenseNet: conexiones densas
- **Lección 3.11.7:** EfficientNet: escalado compuesto

### **Tema 3.12: Redes Neuronales Recurrentes**
- **Lección 3.12.1:** Fundamentos de RNN
- **Lección 3.12.2:** Problema del gradiente que desaparece
- **Lección 3.12.3:** LSTM (Long Short-Term Memory)
- **Lección 3.12.4:** GRU (Gated Recurrent Unit)
- **Lección 3.12.5:** Bidirectional RNN
- **Lección 3.12.6:** Aplicaciones de RNN

### **Tema 3.13: Transfer Learning**
- **Lección 3.13.1:** Concepto de Transfer Learning
- **Lección 3.13.2:** Feature extraction vs Fine-tuning
- **Lección 3.13.3:** Modelos pre-entrenados: ImageNet
- **Lección 3.13.4:** Fine-tuning estratégico por capas
- **Lección 3.13.5:** Domain adaptation

### **Tema 3.14: Data Augmentation para imágenes**
- **Lección 3.14.1:** Técnicas básicas: flip, rotate, crop
- **Lección 3.14.2:** Transformaciones de color
- **Lección 3.14.3:** Augmentation avanzado: CutOut, MixUp
- **Lección 3.14.4:** AutoAugment y RandAugment
- **Lección 3.14.5:** Implementación con TensorFlow y PyTorch

### **Tema 3.15: GPU computing**
- **Lección 3.15.1:** Arquitectura de GPUs para deep learning
- **Lección 3.15.2:** CUDA y cuDNN
- **Lección 3.15.3:** Configuración de TensorFlow con GPU
- **Lección 3.15.4:** Configuración de PyTorch con GPU
- **Lección 3.15.5:** Gestión de memoria GPU
- **Lección 3.15.6:** Multi-GPU training

### **Tema 3.16: Entrenamiento distribuido**
- **Lección 3.16.1:** Data parallelism
- **Lección 3.16.2:** Model parallelism
- **Lección 3.16.3:** Distributed training con TensorFlow
- **Lección 3.16.4:** Distributed training con PyTorch (DDP)
- **Lección 3.16.5:** Mixed precision training

### **Tema 3.17: Debugging de redes neuronales**
- **Lección 3.17.1:** Diagnóstico de problemas comunes
- **Lección 3.17.2:** Visualización de activaciones
- **Lección 3.17.3:** Análisis de gradientes
- **Lección 3.17.4:** TensorBoard para monitorización
- **Lección 3.17.5:** Técnicas de debugging sistemático

---

## **BLOQUE 4: COMPUTER VISION**

### **Tema 4.1: Fundamentos de procesamiento de imágenes**
- **Lección 4.1.1:** Representación digital de imágenes
- **Lección 4.1.2:** Espacios de color: RGB, HSV, LAB
- **Lección 4.1.3:** OpenCV: introducción y operaciones básicas
- **Lección 4.1.4:** Filtros y convoluciones
- **Lección 4.1.5:** Detección de bordes: Sobel, Canny
- **Lección 4.1.6:** Transformaciones geométricas

### **Tema 4.2: Clasificación de imágenes**
- **Lección 4.2.1:** Pipeline completo de clasificación
- **Lección 4.2.2:** Dataset preparation e aumentación
- **Lección 4.2.3:** Arquitecturas modernas para clasificación
- **Lección 4.2.4:** Fine-tuning de modelos pre-entrenados
- **Lección 4.2.5:** Evaluación y métricas específicas
- **Lección 4.2.6:** Caso práctico: clasificador de imágenes médicas

### **Tema 4.3: Detección de objetos - Fundamentos**
- **Lección 4.3.1:** Diferencia entre clasificación y detección
- **Lección 4.3.2:** Bounding boxes y anotaciones
- **Lección 4.3.3:** Intersection over Union (IoU)
- **Lección 4.3.4:** Non-Maximum Suppression (NMS)
- **Lección 4.3.5:** Mean Average Precision (mAP)

### **Tema 4.3: Arquitecturas de detección - R-CNN family**
- **Lección 4.4.1:** R-CNN: region-based CNN
- **Lección 4.4.2:** Fast R-CNN: mejoras de velocidad
- **Lección 4.4.3:** Faster R-CNN: Region Proposal Network
- **Lección 4.4.4:** Mask R-CNN para segmentación
- **Lección 4.4.5:** Feature Pyramid Networks (FPN)

### **Tema 4.5: YOLO (You Only Look Once)**
- **Lección 4.5.1:** YOLO: arquitectura one-stage
- **Lección 4.5.2:** YOLOv3 y YOLOv4
- **Lección 4.5.3:** YOLOv5: implementación en PyTorch
- **Lección 4.5.4:** YOLOv8: última generación
- **Lección 4.5.5:** Training custom YOLO models
- **Lección 4.5.6:** Optimización para tiempo real

### **Tema 4.6: Otras arquitecturas de detección**
- **Lección 4.6.1:** SSD (Single Shot Detector)
- **Lección 4.6.2:** RetinaNet y Focal Loss
- **Lección 4.6.3:** EfficientDet
- **Lección 4.6.4:** Comparativa de arquitecturas

### **Tema 4.7: Segmentación semántica**
- **Lección 4.7.1:** Conceptos de segmentación semántica
- **Lección 4.7.2:** Fully Convolutional Networks (FCN)
- **Lección 4.7.3:** U-Net: arquitectura encoder-decoder
- **Lección 4.7.4:** DeepLab y dilated convolutions
- **Lección 4.7.5:** Métricas: IoU, Dice coefficient
- **Lección 4.7.6:** Aplicaciones médicas de segmentación

### **Tema 4.8: Segmentación de instancias**
- **Lección 4.8.1:** Diferencia entre segmentación semántica e instancias
- **Lección 4.8.2:** Mask R-CNN en profundidad
- **Lección 4.8.3:** YOLACT para segmentación en tiempo real
- **Lección 4.8.4:** Aplicaciones prácticas

### **Tema 4.9: Reconocimiento facial**
- **Lección 4.9.1:** Face detection: Haar Cascades y MTCNN
- **Lección 4.9.2:** Face alignment y landmarks
- **Lección 4.9.3:** Face embeddings: FaceNet, ArcFace
- **Lección 4.9.4:** Face verification vs identification
- **Lección 4.9.5:** Aplicaciones y consideraciones de privacidad

### **Tema 4.10: Vision Transformers**
- **Lección 4.10.1:** Transformers aplicados a visión (ViT)
- **Lección 4.10.2:** Patch embeddings
- **Lección 4.10.3:** Swin Transformer
- **Lección 4.10.4:** DETR: detection con transformers
- **Lección 4.10.5:** ViT vs CNN: comparativa

### **Tema 4.11: Autoencoders y VAE**
- **Lección 4.11.1:** Arquitectura de Autoencoders
- **Lección 4.11.2:** Autoencoders para compresión
- **Lección 4.11.3:** Denoising Autoencoders
- **Lección 4.11.4:** Variational Autoencoders (VAE)
- **Lección 4.11.5:** Latent space y generación

### **Tema 4.12: Generative Adversarial Networks (GANs)**
- **Lección 4.12.1:** Arquitectura de GANs: generator y discriminator
- **Lección 4.12.2:** Entrenamiento adversarial
- **Lección 4.12.3:** Mode collapse y soluciones
- **Lección 4.12.4:** DCGAN: Deep Convolutional GAN
- **Lección 4.12.5:** StyleGAN y StyleGAN2
- **Lección 4.12.6:** Conditional GANs
- **Lección 4.12.7:** CycleGAN para image-to-image translation

### **Tema 4.13: Modelos de difusión**
- **Lección 4.13.1:** Fundamentos de diffusion models
- **Lección 4.13.2:** Denoising Diffusion Probabilistic Models (DDPM)
- **Lección 4.13.3:** Stable Diffusion: arquitectura
- **Lección 4.13.4:** Latent Diffusion Models
- **Lección 4.13.5:** DALL-E: text-to-image
- **Lección 4.13.6:** Aplicaciones creativas

### **Tema 4.14: Modelos multimodales vision-language**
- **Lección 4.14.1:** CLIP: Contrastive Language-Image Pre-training
- **Lección 4.14.2:** Zero-shot image classification con CLIP
- **Lección 4.14.3:** Image captioning
- **Lección 4.14.4:** Visual Question Answering (VQA)
- **Lección 4.14.5:** BLIP y BLIP-2

### **Tema 4.15: OCR y análisis de documentos**
- **Lección 4.15.1:** Optical Character Recognition: fundamentos
- **Lección 4.15.2:** Tesseract OCR
- **Lección 4.15.3:** Deep learning para OCR
- **Lección 4.15.4:** Document layout analysis
- **Lección 4.15.5:** Table extraction y form understanding
- **Lección 4.15.6:** Aplicaciones en automatización

### **Tema 4.16: Aplicaciones en medicina**
- **Lección 4.16.1:** Clasificación de imágenes médicas
- **Lección 4.16.2:** Detección de patologías en radiografías
- **Lección 4.16.3:** Segmentación de órganos y tumores
- **Lección 4.16.4:** Análisis de imágenes histopatológicas
- **Lección 4.16.5:** Consideraciones éticas y regulatorias

### **Tema 4.17: Aplicaciones en retail y seguridad**
- **Lección 4.17.1:** Visual search y product recognition
- **Lección 4.17.2:** Conteo de personas y análisis de flujo
- **Lección 4.17.3:** Sistemas de vigilancia inteligente
- **Lección 4.17.4:** Quality control en manufactura
- **Lección 4.17.5:** Detección de defectos industriales

---

## **BLOQUE 5: PROCESAMIENTO DEL LENGUAJE NATURAL (NLP)**

### **Tema 5.1: Fundamentos de NLP**
- **Lección 5.1.1:** Introducción al procesamiento del lenguaje natural
- **Lección 5.1.2:** Desafíos del lenguaje natural
- **Lección 5.1.3:** Niveles de análisis lingüístico
- **Lección 5.1.4:** Aplicaciones de NLP en la industria
- **Lección 5.1.5:** Recursos y datasets de NLP

### **Tema 5.2: Preprocesamiento de texto**
- **Lección 5.2.1:** Tokenización: palabras, subpalabras, caracteres
- **Lección 5.2.2:** Stemming y Lemmatization
- **Lección 5.2.3:** Stopwords y su eliminación
- **Lección 5.2.4:** Normalización de texto
- **Lección 5.2.5:** Manejo de puntuación y caracteres especiales
- **Lección 5.2.6:** Bibliotecas: NLTK, spaCy, TextBlob

### **Tema 5.3: Representaciones clásicas de texto**
- **Lección 5.3.1:** Bag of Words (BoW)
- **Lección 5.3.2:** N-gramas
- **Lección 5.3.3:** TF-IDF (Term Frequency-Inverse Document Frequency)
- **Lección 5.3.4:** Limitaciones de métodos clásicos
- **Lección 5.3.5:** Implementación práctica

### **Tema 5.4: Word Embeddings**
- **Lección 5.4.1:** Concepto de word embeddings
- **Lección 5.4.2:** Word2Vec: CBOW y Skip-gram
- **Lección 5.4.3:** GloVe: Global Vectors
- **Lección 5.4.4:** FastText: subword embeddings
- **Lección 5.4.5:** Evaluación de word embeddings
- **Lección 5.4.6:** Visualización de embeddings
- **Lección 5.4.7:** Analogías y aritmética de palabras

### **Tema 5.5: Arquitectura Transformer**
- **Lección 5.5.1:** Limitaciones de RNN y LSTM
- **Lección 5.5.2:** Self-Attention mechanism
- **Lección 5.5.3:** Multi-Head Attention
- **Lección 5.5.4:** Positional Encoding
- **Lección 5.5.5:** Arquitectura completa del Transformer
- **Lección 5.5.6:** Encoder y Decoder stacks

### **Tema 5.6: BERT y modelos encoder**
- **Lección 5.6.1:** BERT: Bidirectional Encoder Representations
- **Lección 5.6.2:** Pre-training: Masked Language Modeling
- **Lección 5.6.3:** Next Sentence Prediction
- **Lección 5.6.4:** Fine-tuning BERT para tareas específicas
- **Lección 5.6.5:** RoBERTa: optimizaciones de BERT
- **Lección 5.6.6:** DistilBERT: destilación de conocimiento
- **Lección 5.6.7:** ALBERT: factorización de parámetros

### **Tema 5.7: GPT y modelos decoder**
- **Lección 5.7.1:** GPT: Generative Pre-trained Transformer
- **Lección 5.7.2:** Autoregressive language modeling
- **Lección 5.7.3:** GPT-2: escalado del modelo
- **Lección 5.7.4:** GPT-3 y few-shot learning
- **Lección 5.7.5:** GPT-4: capacidades multimodales
- **Lección 5.7.6:** Comparativa BERT vs GPT

### **Tema 5.8: T5, BART y modelos seq2seq**
- **Lección 5.8.1:** T5: Text-to-Text Transfer Transformer
- **Lección 5.8.2:** Unified text-to-text framework
- **Lección 5.8.3:** BART: denoising autoencoder
- **Lección 5.8.4:** Aplicaciones de modelos seq2seq
- **Lección 5.8.5:** Fine-tuning para tareas específicas

### **Tema 5.9: Named Entity Recognition (NER)**
- **Lección 5.9.1:** Concepto y tipos de entidades
- **Lección 5.9.2:** Enfoques clásicos: CRF, HMM
- **Lección 5.9.3:** NER con BERT y transformers
- **Lección 5.9.4:** Anotación de datos para NER
- **Lección 5.9.5:** Evaluación con F1-score por entidad
- **Lección 5.9.6:** Aplicaciones en extracción de información

### **Tema 5.10: Análisis de sentimientos**
- **Lección 5.10.1:** Clasificación de polaridad: positivo/negativo/neutral
- **Lección 5.10.2:** Aspect-based sentiment analysis
- **Lección 5.10.3:** Fine-grained sentiment analysis
- **Lección 5.10.4:** Datasets: IMDb, Twitter, Amazon reviews
- **Lección 5.10.5:** Transfer learning para sentiment analysis
- **Lección 5.10.6:** Aplicaciones en social media monitoring

### **Tema 5.11: Clasificación de textos**
- **Lección 5.11.1:** Topic classification
- **Lección 5.11.2:** Intent detection
- **Lección 5.11.3:** Spam detection
- **Lección 5.11.4:** Zero-shot classification
- **Lección 5.11.5:** Few-shot classification
- **Lección 5.11.6:** Multi-label classification

### **Tema 5.12: Question Answering**
- **Lección 5.12.1:** Extractive QA vs Generative QA
- **Lección 5.12.2:** SQuAD dataset y benchmark
- **Lección 5.12.3:** Fine-tuning BERT para QA
- **Lección 5.12.4:** Open-domain QA
- **Lección 5.12.5:** Conversational QA
- **Lección 5.12.6:** Knowledge-grounded QA

### **Tema 5.13: Retrieval-Augmented Generation (RAG)**
- **Lección 5.13.1:** Concepto y arquitectura de RAG
- **Lección 5.13.2:** Dense retrieval con embeddings
- **Lección 5.13.3:** Vector databases: FAISS, Pinecone, Weaviate
- **Lección 5.13.4:** Chunking strategies
- **Lección 5.13.5:** Re-ranking de documentos
- **Lección 5.13.6:** Implementación end-to-end de RAG
- **Lección 5.13.7:** Evaluación de sistemas RAG

### **Tema 5.14: Fine-tuning de modelos de lenguaje**
- **Lección 5.14.1:** Full fine-tuning vs parameter-efficient fine-tuning
- **Lección 5.14.2:** Preparación de datasets para fine-tuning
- **Lección 5.14.3:** Hugging Face Transformers library
- **Lección 5.14.4:** Training loops y optimización
- **Lección 5.14.5:** LoRA (Low-Rank Adaptation)
- **Lección 5.14.6:** QLoRA: quantized LoRA
- **Lección 5.14.7:** PEFT (Parameter-Efficient Fine-Tuning)

### **Tema 5.15: Traducción automática**
- **Lección 5.15.1:** Historia de la traducción automática
- **Lección 5.15.2:** Neural Machine Translation (NMT)
- **Lección 5.15.3:** Attention mechanism en traducción
- **Lección 5.15.4:** Transformer para traducción
- **Lección 5.15.5:** Multilingual models
- **Lección 5.15.6:** Evaluación: BLEU, METEOR, BERTScore

### **Tema 5.16: Sistemas multilingües**
- **Lección 5.16.1:** Cross-lingual transfer
- **Lección 5.16.2:** mBERT y XLM-RoBERTa
- **Lección 5.16.3:** Zero-shot cross-lingual learning
- **Lección 5.16.4:** Language detection
- **Lección 5.16.5:** Code-switching y multilingual data

### **Tema 5.17: Chatbots y asistentes conversacionales**
- **Lección 5.17.1:** Arquitectura de chatbots
- **Lección 5.17.2:** Intent classification y entity extraction
- **Lección 5.17.3:** Dialog state tracking
- **Lección 5.17.4:** Response generation
- **Lección 5.17.5:** Rasa y frameworks de chatbots
- **Lección 5.17.6:** Conversational AI con LLMs
- **Lección 5.17.7:** Evaluación de chatbots

### **Tema 5.18: Evaluación de modelos de lenguaje**
- **Lección 5.18.1:** Perplexity
- **Lección 5.18.2:** BLEU score para generación
- **Lección 5.18.3:** ROUGE para summarization
- **Lección 5.18.4:** BERTScore
- **Lección 5.18.5:** Human evaluation
- **Lección 5.18.6:** Benchmarks: GLUE, SuperGLUE

---

## **BLOQUE 6: INTELIGENCIA ARTIFICIAL GENERATIVA Y LLMs**

### **Tema 6.1: Introducción a IA Generativa**
- **Lección 6.1.1:** Qué es la IA Generativa
- **Lección 6.1.2:** Historia y evolución
- **Lección 6.1.3:** Tipos de modelos generativos
- **Lección 6.1.4:** Aplicaciones en industria y creatividad
- **Lección 6.1.5:** Impacto social y económico

### **Tema 6.2: Large Language Models - Arquitectura**
- **Lección 6.2.1:** Scaling laws en LLMs
- **Lección 6.2.2:** Arquitectura de LLMs modernos
- **Lección 6.2.3:** Pre-training de LLMs
- **Lección 6.2.4:** Tokenización en LLMs: BPE, WordPiece
- **Lección 6.2.5:** Context window y atención eficiente

### **Tema 6.3: Modelos LLM actuales**
- **Lección 6.3.1:** GPT-4 y GPT-4 Turbo: características
- **Lección 6.3.2:** Claude (Anthropic): arquitectura y capacidades
- **Lección 6.3.3:** Llama 2 y Llama 3: modelos open source
- **Lección 6.3.4:** Gemini (Google): multimodalidad nativa
- **Lección 6.3.5:** Mistral: modelos europeos eficientes
- **Lección 6.3.6:** Comparativa de LLMs comerciales

### **Tema 6.4: Prompt Engineering - Fundamentos**
- **Lección 6.4.1:** Qué es prompt engineering
- **Lección 6.4.2:** Anatomía de un buen prompt
- **Lección 6.4.3:** Zero-shot prompting
- **Lección 6.4.4:** Few-shot prompting
- **Lección 6.4.5:** One-shot prompting
- **Lección 6.4.6:** Role prompting

### **Tema 6.5: Prompt Engineering - Técnicas avanzadas**
- **Lección 6.5.1:** Chain-of-Thought (CoT) prompting
- **Lección 6.5.2:** Tree of Thoughts
- **Lección 6.5.3:** ReAct: Reasoning + Acting
- **Lección 6.5.4:** Self-Consistency
- **Lección 6.5.5:** Generated Knowledge prompting
- **Lección 6.5.6:** Prompt chaining
- **Lección 6.5.7:** Automatic prompt engineering

### **Tema 6.6: Fine-tuning de LLMs - Conceptos**
- **Lección 6.6.1:** Cuándo hacer fine-tuning vs prompting
- **Lección 6.6.2:** Full fine-tuning: proceso y recursos
- **Lección 6.6.3:** Instruction tuning
- **Lección 6.6.4:** Supervised fine-tuning (SFT)
- **Lección 6.6.5:** Preparación de datasets de instrucciones

### **Tema 6.7: Fine-tuning eficiente**
- **Lección 6.7.1:** LoRA en profundidad
- **Lección 6.7.2:** QLoRA: fine-tuning con 4-bit quantization
- **Lección 6.7.3:** Prefix tuning y P-tuning
- **Lección 6.7.4:** Adapter layers
- **Lección 6.7.5:** Comparativa de métodos PEFT
- **Lección 6.7.6:** Implementación práctica con Hugging Face

### **Tema 6.8: RAG en profundidad**
- **Lección 6.8.1:** Arquitectura detallada de RAG
- **Lección 6.8.2:** Embeddings para retrieval: sentence-transformers
- **Lección 6.8.3:** Chunk size y overlap strategies
- **Lección 6.8.4:** Hybrid search: dense + sparse
- **Lección 6.8.5:** Metadata filtering
- **Lección 6.8.6:** Parent document retrieval
- **Lección 6.8.7:** Self-query retrievers

### **Tema 6.9: Vector databases**
- **Lección 6.9.1:** FAISS: Facebook AI Similarity Search
- **Lección 6.9.2:** Pinecone: managed vector database
- **Lección 6.9.3:** Weaviate: vector search engine
- **Lección 6.9.4:** Qdrant y Milvus
- **Lección 6.9.5:** ChromaDB para desarrollo local
- **Lección 6.9.6:** Comparativa y selección

### **Tema 6.10: LangChain y frameworks**
- **Lección 6.10.1:** Introducción a LangChain
- **Lección 6.10.2:** Chains y sequential chains
- **Lección 6.10.3:** Agents y tools
- **Lección 6.10.4:** Memory en conversaciones
- **Lección 6.10.5:** LlamaIndex para RAG
- **Lección 6.10.6:** Haystack framework

### **Tema 6.11: Modelos multimodales**
- **Lección 6.11.1:** Concepto de multimodalidad
- **Lección 6.11.2:** GPT-4V (Vision): imagen + texto
- **Lección 6.11.3:** LLaVA: Large Language and Vision Assistant
- **Lección 6.11.4:** Flamingo y otras arquitecturas
- **Lección 6.11.5:** Audio + texto: Whisper integrado
- **Lección 6.11.6:** Video understanding

### **Tema 6.12: Generación de código con IA**
- **Lección 6.12.1:** GitHub Copilot: arquitectura y uso
- **Lección 6.12.2:** CodeLlama: fine-tuning de Llama para código
- **Lección 6.12.3:** StarCoder y modelos open source
- **Lección 6.12.4:** Code generation vs code completion
- **Lección 6.12.5:** Test generation automático
- **Lección 6.12.6:** Code review con LLMs
- **Lección 6.12.7:** Debugging asistido por IA

### **Tema 6.13: Generación de audio**
- **Lección 6.13.1:** Whisper: speech-to-text de OpenAI
- **Lección 6.13.2:** Text-to-Speech con modelos neurales
- **Lección 6.13.3:** Voice cloning
- **Lección 6.13.4:** MusicGen: generación de música
- **Lección 6.13.5:** AudioCraft de Meta
- **Lección 6.13.6:** Aplicaciones en podcasting y multimedia

### **Tema 6.14: Generación de video**
- **Lección 6.14.1:** Sora de OpenAI: conceptos y capacidades
- **Lección 6.14.2:** Runway Gen-2
- **Lección 6.14.3:** Pika Labs
- **Lección 6.14.4:** Arquitecturas de video generation
- **Lección 6.14.5:** Text-to-video y image-to-video
- **Lección 6.14.6:** Limitaciones actuales

### **Tema 6.15: Alucinaciones en LLMs**
- **Lección 6.15.1:** Qué son las alucinaciones
- **Lección 6.15.2:** Causas de alucinaciones
- **Lección 6.15.3:** Detección de alucinaciones
- **Lección 6.15.4:** Técnicas de mitigación
- **Lección 6.15.5:** Fact-checking automático
- **Lección 6.15.6:** Calibración de confianza

### **Tema 6.16: Limitaciones de modelos generativos**
- **Lección 6.16.1:** Sesgos en modelos generativos
- **Lección 6.16.2:** Limitaciones de razonamiento
- **Lección 6.16.3:** Problemas de actualización de conocimiento
- **Lección 6.16.4:** Context window limitations
- **Lección 6.16.5:** Costes computacionales
- **Lección 6.16.6:** Consideraciones de seguridad

### **Tema 6.17: Detección de contenido sintético**
- **Lección 6.17.1:** AI-generated text detection
- **Lección 6.17.2:** Watermarking en contenido generado
- **Lección 6.17.3:** Deepfake detection
- **Lección 6.17.4:** Herramientas de verificación
- **Lección 6.17.5:** Implicaciones legales y éticas

---

## **BLOQUE 7: REINFORCEMENT LEARNING Y APLICACIONES ESPECIALIZADAS**

### **Tema 7.1: Fundamentos de Reinforcement Learning**
- **Lección 7.1.1:** Introducción al aprendizaje por refuerzo
- **Lección 7.1.2:** Agentes, entornos y recompensas
- **Lección 7.1.3:** Estados, acciones y políticas
- **Lección 7.1.4:** Función de valor y función Q
- **Lección 7.1.5:** Exploitation vs Exploration
- **Lección 7.1.6:** Ecuación de Bellman

### **Tema 7.2: Procesos de decisión de Markov**
- **Lección 7.2.1:** Markov Decision Processes (MDP)
- **Lección 7.2.2:** Partially Observable MDPs (POMDP)
- **Lección 7.2.3:** Reward shaping
- **Lección 7.2.4:** Discount factor
- **Lección 7.2.5:** Formulación matemática

### **Tema 7.3: Algoritmos clásicos de RL**
- **Lección 7.3.1:** Q-Learning
- **Lección 7.3.2:** SARSA (State-Action-Reward-State-Action)
- **Lección 7.3.3:** Temporal Difference Learning
- **Lección 7.3.4:** Monte Carlo methods
- **Lección 7.3.5:** Dynamic Programming

### **Tema 7.4: Deep Reinforcement Learning**
- **Lección 7.4.1:** Deep Q-Networks (DQN)
- **Lección 7.4.2:** Experience replay
- **Lección 7.4.3:** Target networks
- **Lección 7.4.4:** Double DQN
- **Lección 7.4.5:** Dueling DQN
- **Lección 7.4.6:** Rainbow DQN

### **Tema 7.5: Policy Gradient methods**
- **Lección 7.5.1:** Policy-based vs Value-based methods
- **Lección 7.5.2:** REINFORCE algorithm
- **Lección 7.5.3:** Actor-Critic methods
- **Lección 7.5.4:** A2C (Advantage Actor-Critic)
- **Lección 7.5.5:** A3C (Asynchronous Advantage Actor-Critic)

### **Tema 7.6: Proximal Policy Optimization**
- **Lección 7.6.1:** PPO: motivación y ventajas
- **Lección 7.6.2:** Clipped objective function
- **Lección 7.6.3:** Trust Region Policy Optimization (TRPO)
- **Lección 7.6.4:** Implementación de PPO
- **Lección 7.6.5:** Aplicaciones de PPO

### **Tema 7.7: RLHF (Reinforcement Learning from Human Feedback)**
- **Lección 7.7.1:** Concepto de RLHF
- **Lección 7.7.2:** Reward modeling con feedback humano
- **Lección 7.7.3:** RLHF en LLMs: ChatGPT y Claude
- **Lección 7.7.4:** Constitutional AI
- **Lección 7.7.5:** Direct Preference Optimization (DPO)

### **Tema 7.8: Entornos de simulación**
- **Lección 7.8.1:** OpenAI Gym: introducción
- **Lección 7.8.2:** Gymnasium (nuevo Gym)
- **Lección 7.8.3:** Unity ML-Agents
- **Lección 7.8.4:** PyBullet para robótica
- **Lección 7.8.5:** Custom environments

### **Tema 7.9: Aplicaciones de RL**
- **Lección 7.9.1:** RL en robótica: navegación y manipulación
- **Lección 7.9.2:** RL en juegos: AlphaGo, AlphaZero
- **Lección 7.9.3:** OpenAI Five y Dota 2
- **Lección 7.9.4:** RL en control y optimización
- **Lección 7.9.5:** RL en finanzas: trading algorítmico
- **Lección 7.9.6:** RL en sistemas de recomendación

### **Tema 7.10: Time Series Forecasting**
- **Lección 7.10.1:** Características de series temporales
- **Lección 7.10.2:** ARIMA y métodos estadísticos
- **Lección 7.10.3:** LSTM para forecasting
- **Lección 7.10.4:** Temporal Convolutional Networks (TCN)
- **Lección 7.10.5:** Transformers para series temporales
- **Lección 7.10.6:** Prophet de Facebook
- **Lección 7.10.7:** Evaluación de forecasting models

### **Tema 7.11: Recommender Systems**
- **Lección 7.11.1:** Fundamentos de sistemas de recomendación
- **Lección 7.11.2:** Collaborative filtering: user-based y item-based
- **Lección 7.11.3:** Matrix factorization
- **Lección 7.11.4:** Content-based filtering
- **Lección 7.11.5:** Hybrid recommenders
- **Lección 7.11.6:** Deep learning para recomendación
- **Lección 7.11.7:** Neural Collaborative Filtering
- **Lección 7.11.8:** Evaluación: precision@k, recall@k, NDCG

### **Tema 7.12: Graph Neural Networks**
- **Lección 7.12.1:** Introducción a grafos en ML
- **Lección 7.12.2:** Graph Convolutional Networks (GCN)
- **Lección 7.12.3:** GraphSAGE
- **Lección 7.12.4:** Graph Attention Networks (GAT)
- **Lección 7.12.5:** Aplicaciones: social networks, molecular graphs
- **Lección 7.12.6:** PyTorch Geometric

### **Tema 7.13: AutoML**
- **Lección 7.13.1:** Concepto y motivación de AutoML
- **Lección 7.13.2:** Neural Architecture Search (NAS)
- **Lección 7.13.3:** Auto-sklearn
- **Lección 7.13.4:** TPOT: Tree-based Pipeline Optimization
- **Lección 7.13.5:** H2O AutoML
- **Lección 7.13.6:** Google AutoML
- **Lección 7.13.7:** Limitaciones de AutoML

---

## **BLOQUE 8: MLOPS, PRODUCCIÓN Y PROYECTO INTEGRADOR**

### **Tema 8.1: Ciclo de vida de proyectos ML**
- **Lección 8.1.1:** Etapas de un proyecto de ML
- **Lección 8.1.2:** De notebook a producción
- **Lección 8.1.3:** Technical debt en ML systems
- **Lección 8.1.4:** ML Canvas: planificación de proyectos
- **Lección 8.1.5:** Roles en equipos de ML

### **Tema 8.2: Versionado y experimentación**
- **Lección 8.2.1:** Importancia del experiment tracking
- **Lección 8.2.2:** MLflow: tracking, projects, models
- **Lección 8.2.3:** Weights & Biases (wandb)
- **Lección 8.2.4:** Neptune.ai
- **Lección 8.2.5:** Comparativa de herramientas
- **Lección 8.2.6:** Best practices en experimentación

### **Tema 8.3: Feature Stores**
- **Lección 8.3.1:** Concepto y arquitectura de Feature Store
- **Lección 8.3.2:** Feast: open source feature store
- **Lección 8.3.3:** Tecton: feature platform empresarial
- **Lección 8.3.4:** Hopsworks Feature Store
- **Lección 8.3.5:** Integración con pipelines de Big Data
- **Lección 8.3.6:** Online vs offline feature serving

### **Tema 8.4: Containerización de modelos**
- **Lección 8.4.1:** Docker para ML: conceptos básicos
- **Lección 8.4.2:** Dockerfile para aplicaciones ML
- **Lección 8.4.3:** Docker Compose para multi-container
- **Lección 8.4.4:** Optimización de imágenes Docker
- **Lección 8.4.5:** GPU support en containers
- **Lección 8.4.6:** Docker Registry y gestión de imágenes

### **Tema 8.5: Kubernetes para ML**
- **Lección 8.5.1:** Fundamentos de Kubernetes
- **Lección 8.5.2:** Pods, Services, Deployments
- **Lección 8.5.3:** ConfigMaps y Secrets
- **Lección 8.5.4:** Horizontal Pod Autoscaling
- **Lección 8.5.5:** Kubeflow: ML on Kubernetes
- **Lección 8.5.6:** Seldon Core para model serving

### **Tema 8.6: Pipelines de ML**
- **Lección 8.6.1:** Diseño de pipelines de ML
- **Lección 8.6.2:** Airflow para ML workflows
- **Lección 8.6.3:** Kubeflow Pipelines
- **Lección 8.6.4:** Vertex AI Pipelines (GCP)
- **Lección 8.6.5:** AWS SageMaker Pipelines
- **Lección 8.6.6:** Azure ML Pipelines
- **Lección 8.6.7:** Pipeline orchestration best practices

### **Tema 8.7: Model Serving - APIs**
- **Lección 8.7.1:** FastAPI para ML: introducción
- **Lección 8.7.2:** Diseño de endpoints RESTful
- **Lección 8.7.3:** Validación de requests con Pydantic
- **Lección 8.7.4:** Async inference con FastAPI
- **Lección 8.7.5:** Flask para ML (legacy)
- **Lección 8.7.6:** gRPC para high-performance serving

### **Tema 8.8: Model Serving - Frameworks**
- **Lección 8.8.1:** TensorFlow Serving
- **Lección 8.8.2:** TorchServe
- **Lección 8.8.3:** Triton Inference Server (NVIDIA)
- **Lección 8.8.4:** BentoML
- **Lección 8.8.5:** Ray Serve
- **Lección 8.8.6:** Comparativa de frameworks

### **Tema 8.9: Monitorización de modelos**
- **Lección 8.9.1:** Métricas de producción
- **Lección 8.9.2:** Logging centralizado para ML
- **Lección 8.9.3:** Prometheus y Grafana
- **Lección 8.9.4:** Application Performance Monitoring (APM)
- **Lección 8.9.5:** Alerting y incident management
- **Lección 8.9.6:** Observability en sistemas ML

### **Tema 8.10: Data Drift y Model Drift**
- **Lección 8.10.1:** Tipos de drift: data, concept, prediction
- **Lección 8.10.2:** Detección de data drift
- **Lección 8.10.3:** Detección de model drift
- **Lección 8.10.4:** Evidently AI para drift detection
- **Lección 8.10.5:** Alibi Detect
- **Lección 8.10.6:** Estrategias de retraining

### **Tema 8.11: A/B Testing**
- **Lección 8.11.1:** Fundamentos de A/B testing
- **Lección 8.11.2:** Diseño de experimentos
- **Lección 8.11.3:** Significancia estadística
- **Lección 8.11.4:** Multi-armed bandits
- **Lección 8.11.5:** Shadow deployment y canary releases
- **Lección 8.11.6:** Herramientas para A/B testing

### **Tema 8.12: CI/CD para Machine Learning**
- **Lección 8.12.1:** Continuous Integration para ML
- **Lección 8.12.2:** Continuous Delivery/Deployment
- **Lección 8.12.3:** GitHub Actions para ML
- **Lección 8.12.4:** GitLab CI/CD
- **Lección 8.12.5:** Testing de modelos ML
- **Lección 8.12.6:** Model validation gates

### **Tema 8.13: Optimización de inferencia**
- **Lección 8.13.1:** Model quantization: INT8, FP16
- **Lección 8.13.2:** Pruning: eliminación de pesos
- **Lección 8.13.3:** Knowledge distillation
- **Lección 8.13.4:** ONNX: formato de intercambio
- **Lección 8.13.5:** TensorRT para GPUs NVIDIA
- **Lección 8.13.6:** OpenVINO para CPUs Intel

### **Tema 8.14: Edge AI**
- **Lección 8.14.1:** ML en dispositivos edge
- **Lección 8.14.2:** TensorFlow Lite
- **Lección 8.14.3:** PyTorch Mobile
- **Lección 8.14.4:** Core ML para iOS
- **Lección 8.14.5:** ML Kit para Android
- **Lección 8.14.6:** Optimización para edge devices

### **Tema 8.15: Explainable AI (XAI)**
- **Lección 8.15.1:** Importancia de la interpretabilidad
- **Lección 8.15.2:** SHAP (SHapley Additive exPlanations)
- **Lección 8.15.3:** LIME (Local Interpretable Model-agnostic Explanations)
- **Lección 8.15.4:** Attention visualization en transformers
- **Lección 8.15.5:** Feature importance global y local
- **Lección 8.15.6:** Partial Dependence Plots
- **Lección 8.15.7:** Counterfactual explanations

### **Tema 8.16: Sesgos y fairness**
- **Lección 8.16.1:** Tipos de sesgos en ML
- **Lección 8.16.2:** Fairness metrics
- **Lección 8.16.3:** Detección de sesgos en datasets
- **Lección 8.16.4:** Detección de sesgos en modelos
- **Lección 8.16.5:** Técnicas de mitigación de sesgos
- **Lección 8.16.6:** AI Fairness 360 (IBM)

### **Tema 8.17: Proyecto Integrador - Planificación**
- **Lección 8.17.1:** Selección del problema de negocio
- **Lección 8.17.2:** Definición de objetivos y KPIs
- **Lección 8.17.3:** Análisis de requisitos técnicos
- **Lección 8.17.4:** Diseño de arquitectura Big Data + IA
- **Lección 8.17.5:** Planning y cronograma del proyecto

### **Tema 8.18: Proyecto Integrador - Implementación Data**
- **Lección 8.18.1:** Configuración de infraestructura cloud
- **Lección 8.18.2:** Implementación de data pipelines
- **Lección 8.18.3:** Configuración de Feature Store
- **Lección 8.18.4:** Data quality checks
- **Lección 8.18.5:** Preparación de datasets de entrenamiento

### **Tema 8.19: Proyecto Integrador - Desarrollo de modelos**
- **Lección 8.19.1:** Exploración y análisis de datos
- **Lección 8.19.2:** Feature engineering
- **Lección 8.19.3:** Entrenamiento de modelos baseline
- **Lección 8.19.4:** Experimentación y tuning
- **Lección 8.19.5:** Selección del modelo final
- **Lección 8.19.6:** Validación exhaustiva

### **Tema 8.20: Proyecto Integrador - Despliegue**
- **Lección 8.20.1:** Containerización del modelo
- **Lección 8.20.2:** Implementación de API de inferencia
- **Lección 8.20.3:** Despliegue en Kubernetes
- **Lección 8.20.4:** Configuración de monitorización
- **Lección 8.20.5:** Testing de carga y performance
- **Lección 8.20.6:** Documentación de APIs

### **Tema 8.21: Proyecto Integrador - Evaluación y documentación**
- **Lección 8.21.1:** Evaluación de métricas de negocio
- **Lección 8.21.2:** Análisis de impacto y ROI
- **Lección 8.21.3:** Documentación técnica completa
- **Lección 8.21.4:** Manual de operaciones
- **Lección 8.21.5:** Plan de mantenimiento y evolución
- **Lección 8.21.6:** Lecciones aprendidas

### **Tema 8.22: Proyecto Integrador - Presentación**
- **Lección 8.22.1:** Preparación de presentación ejecutiva
- **Lección 8.22.2:** Storytelling con datos y resultados
- **Lección 8.22.3:** Demo técnica del sistema
- **Lección 8.22.4:** Q&A y defensa técnica
- **Lección 8.22.5:** Feedback y mejoras propuestas

---

**RESUMEN PARTE INTELIGENCIA ARTIFICIAL:**
- **8 Bloques**
- **113 Temas**
- **512 Lecciones**
